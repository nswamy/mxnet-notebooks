{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "We will Download the [data -- fill me up]() and extract it to the current directory.\n",
    "Included in the ``data/names`` directory are 18 text files named as\n",
    "\"[Language].txt\". Each file contains a bunch of names, one name per\n",
    "line, mostly romanized (but we still need to convert from Unicode to\n",
    "ASCII).\n",
    "\n",
    "We'll end up with a dictionary of lists of names per language,\n",
    "``{language: [names ...]}``. The generic variables \"category\" and \"line\"\n",
    "(for language and name in our case) are used for later extensibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import zipfile\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon import rnn\n",
    "from mxnet import gluon, autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change data location below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir_name='./char-rnn-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_data(data_dir_name):\n",
    "    fname = mx.test_utils.download(url='https://download.pytorch.org/tutorial/data.zip', dirname=data_dir_name, overwrite=False)\n",
    "    zip = zipfile.ZipFile(fname)\n",
    "    zip.extractall(data_dir_name)\n",
    "    zip.close()\n",
    "\n",
    "download_data(data_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_input(path):\n",
    "    with open(path + '/train_data.txt', 'r') as td, open(path + '/train_label.txt', 'r') as tl:\n",
    "        train_data = [line.strip('\\n') for line in td.readlines()]\n",
    "        train_label = [line.strip('\\n') for line in tl.readlines()]\n",
    "\n",
    "    with open(path + '/val_data.txt', 'r') as vd, open(path + '/val_label.txt', 'r') as vl:\n",
    "        eval_data = [line.strip('\\n') for line in vd.readlines()]\n",
    "        eval_label = [line.strip('\\n') for line in vl.readlines()]\n",
    "\n",
    "    return train_data, train_label, eval_data, eval_label\n",
    "\n",
    "train_data, train_label, eval_data, eval_label = read_input(data_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = string.ascii_letters + \" .,;'\"\n",
    "vocab_size = len(vocab)\n",
    "train_labels_unique = list(set(train_label))\n",
    "eval_labels_unique = list(set(eval_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_onehot = [mx.nd.one_hot(mx.nd.array([vocab.find(ch) for ch in line]), vocab_size) for line in train_data]\n",
    "train_label_onehot = [mx.nd.one_hot(mx.nd.array([train_labels_unique.index(label)]), len(train_labels_unique)) for label in train_label]\n",
    "num_train_data = len(train_data_onehot)\n",
    "\n",
    "eval_data_onehot = [mx.nd.one_hot(mx.nd.array([vocab.find(ch) for ch in line]), vocab_size) for line in eval_data]\n",
    "eval_label_onehot = [mx.nd.one_hot(mx.nd.array([eval_labels_unique.index(label)]), len(eval_labels_unique)) for label in eval_label]\n",
    "num_eval_data = len(eval_data_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.len 18076 train_label.len 18076\n",
      "train_data.shape (7L, 57L) train_label.shape (1L, 18L)\n",
      "num_train_data: 18076\n",
      "eval_data.len 1998 eval_label.len 1998\n",
      "eval_data.shape (5L, 57L) eval_label.shape (1L, 18L)\n",
      "num_eval_data: 1998\n"
     ]
    }
   ],
   "source": [
    "print('train_data.len', len(train_data_onehot), 'train_label.len', len(train_label_onehot))\n",
    "print('train_data.shape', train_data_onehot[0].shape, 'train_label.shape', train_label_onehot[0].shape)\n",
    "print('num_train_data:', num_train_data)\n",
    "\n",
    "print('eval_data.len', len(eval_data_onehot), 'eval_label.len', len(eval_label_onehot))\n",
    "print('eval_data.shape', eval_data_onehot[0].shape, 'eval_label.shape', eval_label_onehot[0].shape)\n",
    "print('num_eval_data:', num_eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 128\n",
    "num_layers = 2\n",
    "batch_size = 1\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(gluon.Block):\n",
    "    def __init__(self, num_layers, num_hidden, num_output_class, dropout_prob=0.5, **kwargs):\n",
    "        super(RNN, self).__init__(**kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout_prob)            \n",
    "            self.stack = rnn.SequentialRNNCell()\n",
    "            for n in range(num_layers):\n",
    "                self.stack.add(rnn.LSTMCell(hidden_size=num_hidden, prefix='lstm_l%d_'%n))\n",
    "            self.fc = nn.Dense(num_output_class, in_units=num_hidden)\n",
    "\n",
    "    def forward(self, inputs, seq_length):\n",
    "#         print('inputs.shape', inputs.shape)                \n",
    "        lstm_output, hidden = self.stack.unroll(seq_length, inputs, layout='NTC', merge_outputs=True)\n",
    "#         print('lstm_output.shape', lstm_output.shape)        \n",
    "        drop_output = self.drop.forward(lstm_output)\n",
    "#         print('drop_output', drop_output.shape)\n",
    "        \n",
    "        fc_input = mx.nd.sum(drop_output, axis=1)\n",
    "        fc_input = mx.nd.divide(fc_input, seq_length)\n",
    "#         print('fc_input.shape', fc_input.shape)\n",
    "        fc_output = self.fc(fc_input)        \n",
    "#         print('fc_output.shape', fc_output.shape)              \n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNN(num_layers, num_hidden, num_output_class=len(train_labels_unique))\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', {'learning_rate': 0.001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval():\n",
    "    total_loss = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.stack.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=ctx)\n",
    "    \n",
    "    for index in range(num_eval_data):\n",
    "        data = eval_data_onehot[index].as_in_context(ctx)\n",
    "        data = data.reshape(((1,) + data.shape))\n",
    "        \n",
    "        target = eval_label_onehot[index].as_in_context(ctx)\n",
    "        target = mx.nd.split(data=target, axis=0, num_outputs=1, squeeze_axis=True)\n",
    "\n",
    "        output = model.forward(data, data.shape[1])\n",
    "        output = output.reshape((target.shape[0], 1))\n",
    "        \n",
    "        L = mx.nd.softmax_cross_entropy(output, target)\n",
    "\n",
    "        total_loss += mx.nd.sum(L).asscalar()\n",
    "\n",
    "    return total_loss/num_eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "def train(print_every=500):\n",
    "    start = time.time()\n",
    "    for epoch in range(1, num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for iter in range(1, num_train_data):\n",
    "            \n",
    "            hidden = model.stack.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=ctx)\n",
    "            with autograd.record():\n",
    "                index = random.randint(0, num_train_data - 1)\n",
    "                data = train_data_onehot[index].as_in_context(ctx)\n",
    "                # since we do not have a batch size we'll reshape it to a batch_size of 1.\n",
    "                data = data.reshape(((1,) + data.shape))                \n",
    "\n",
    "                target = train_label_onehot[index].as_in_context(ctx)\n",
    "                target = mx.nd.split(data=target, axis=0, num_outputs=1, squeeze_axis=True)\n",
    "#                 print('target_shape.', target.shape)\n",
    "                \n",
    "                output = model.forward(data, data.shape[1])\n",
    "#                 print('forward_output.shape', output.shape)\n",
    "                \n",
    "                output = output.reshape((target.shape[0], 1))\n",
    "    \n",
    "#                 print('forward_output.reshape.shape', output.shape)\n",
    "                \n",
    "                softmax_out = mx.nd.softmax(output, axis=0)\n",
    "                L = mx.nd.softmax_cross_entropy(data=output, label=target)\n",
    "                L.backward()\n",
    "            \n",
    "            trainer.step(batch_size)\n",
    "\n",
    "            total_loss += L.asscalar()\n",
    "        \n",
    "        val_loss = eval()\n",
    "            \n",
    "        losses.append(total_loss)\n",
    "        print('[Epoch %d] Training loss=%f, Val loss=%f, time=%s'%(epoch, total_loss/num_train_data, val_loss, timeSince(start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Training loss=0.490172, Val loss=0.470198, time=6m 26s\n",
      "[Epoch 2] Training loss=0.448390, Val loss=0.470198, time=12m 47s\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 18L)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
       "<NDArray 18 @cpu(0)>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = mx.nd.ones((1,18))\n",
    "print(x.shape)\n",
    "mx.nd.split(data=x,num_outputs=1, axis=0, squeeze_axis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
