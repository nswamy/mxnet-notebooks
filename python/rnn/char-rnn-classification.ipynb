{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "We will Download the [data -- fill me up]() and extract it to the current directory.\n",
    "Included in the ``data/names`` directory are 18 text files named as\n",
    "\"[Language].txt\". Each file contains a bunch of names, one name per\n",
    "line, mostly romanized (but we still need to convert from Unicode to\n",
    "ASCII).\n",
    "\n",
    "We'll end up with a dictionary of lists of names per language,\n",
    "``{language: [names ...]}``. The generic variables \"category\" and \"line\"\n",
    "(for language and name in our case) are used for later extensibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import zipfile\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon import rnn\n",
    "from mxnet import gluon, autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change data location below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir_name='./char-rnn-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_data(data_dir_name):\n",
    "    fname = mx.test_utils.download(url='https://download.pytorch.org/tutorial/data.zip', dirname=data_dir_name, overwrite=False)\n",
    "    zip = zipfile.ZipFile(fname)\n",
    "    zip.extractall(data_dir_name)\n",
    "    zip.close()\n",
    "\n",
    "download_data(data_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_data: 20074\n",
      "num_train: 18067\n",
      "num_eval: 2007\n",
      "num_train_label: 18067\n",
      "num_eval_label: 2007\n",
      "vocab_size: 57\n",
      "num_labels: 18\n"
     ]
    }
   ],
   "source": [
    "all_lines = []\n",
    "with open(data_dir_name + '/data.txt', 'r' ) as d:\n",
    "    all_lines = [l.strip() for l in d.readlines()]\n",
    "\n",
    "import random\n",
    "# we will assume a batch size of 1 and shuffle all lines, typically the shuffling should happen \n",
    "# for each mini-batch to preserve temporal data.\n",
    "random.shuffle(all_lines)\n",
    "\n",
    "all_lines = [line.split(',') for line in all_lines]\n",
    "all_lines = filter(None, all_lines) \n",
    "\n",
    "names_all = [l[0].strip() for l in all_lines]\n",
    "all_lines = filter(None, all_lines) \n",
    "\n",
    "categories_all = [l[1].strip() for l in all_lines]\n",
    "categories_all = filter(None, categories_all) \n",
    "\n",
    "categories = list(set(categories_all))\n",
    "n_categories = len(categories)\n",
    "\n",
    "# we will choose 10% of data for evaluation and 10% for test\n",
    "num_eval = int(0.10 * len(names_all))\n",
    "\n",
    "train_data = names_all[0:-num_eval]\n",
    "eval_data = names_all[-num_eval:] \n",
    "\n",
    "num_train = len(train_data)\n",
    "\n",
    "train_label = categories_all[0:-num_eval]\n",
    "eval_label = categories_all[-num_eval:] \n",
    "\n",
    "vocab = string.ascii_letters + \" .,;'\"\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print('num_data:', len(names_all))\n",
    "print('num_train:', len(train_data))\n",
    "print('num_eval:', len(eval_data))\n",
    "print('num_train_label:', len(train_label))\n",
    "print('num_eval_label:', len(eval_label))\n",
    "\n",
    "print('vocab_size:', vocab_size)\n",
    "print('num_labels:', n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_onehot = [mx.nd.one_hot(mx.nd.array([vocab.find(ch) for ch in line]), vocab_size) for line in train_data]\n",
    "train_label_onehot = [mx.nd.one_hot(mx.nd.array([categories.index(label)]), n_categories) for label in train_label]\n",
    "\n",
    "eval_data_onehot = [mx.nd.one_hot(mx.nd.array([vocab.find(ch) for ch in line]), vocab_size) for line in eval_data]\n",
    "eval_label_onehot = [mx.nd.one_hot(mx.nd.array([categories.index(label)]), n_categories) for label in eval_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.len 18067\n",
      "train_data.shape (7L, 57L)\n",
      "num_train: 18067\n",
      "eval_data.len 2007 eval_label.len 2007\n",
      "eval_data.shape (6L, 57L) eval_label.shape (1L, 18L)\n",
      "num_eval: 2007\n"
     ]
    }
   ],
   "source": [
    "print('train_data.len', len(train_data_onehot))\n",
    "print('train_data.shape', train_data_onehot[0].shape)\n",
    "print('num_train:', num_train)\n",
    "\n",
    "print('eval_data.len', len(eval_data_onehot), 'eval_label.len', len(eval_label_onehot))\n",
    "print('eval_data.shape', eval_data_onehot[0].shape, 'eval_label.shape', eval_label_onehot[0].shape)\n",
    "print('num_eval:', num_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 100\n",
    "num_layers = 2\n",
    "batch_size = 1\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(gluon.Block):\n",
    "    def __init__(self, num_layers, num_hidden, num_output_class, dropout_prob=0.5, **kwargs):\n",
    "        super(RNN, self).__init__(**kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout_prob)            \n",
    "            self.stack = rnn.SequentialRNNCell()\n",
    "            for n in range(num_layers):\n",
    "                self.stack.add(rnn.LSTMCell(hidden_size=num_hidden, prefix='lstm_l%d_'%n))\n",
    "            self.fc = nn.Dense(num_output_class, in_units=num_hidden)\n",
    "\n",
    "    def forward(self, inputs, seq_length):\n",
    "#         print('inputs.shape', inputs.shape)                \n",
    "        lstm_output, hidden = self.stack.unroll(seq_length, inputs, layout='NTC', merge_outputs=True)\n",
    "#         print('lstm_output.shape', lstm_output.shape)        \n",
    "        drop_output = self.drop.forward(lstm_output)\n",
    "#         print('drop_output', drop_output.shape)\n",
    "        \n",
    "        fc_input = mx.nd.sum(drop_output, axis=1)\n",
    "        fc_input = mx.nd.divide(fc_input, seq_length)\n",
    "#         print('fc_input.shape', fc_input.shape)\n",
    "        fc_output = self.fc(fc_input)        \n",
    "#         print('fc_output.shape', fc_output.shape)              \n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(num_layers, num_hidden, num_output_class=n_categories)\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', {'learning_rate': 0.001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval():\n",
    "    total_loss = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.stack.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=ctx)\n",
    "    \n",
    "    for index in range(num_eval):\n",
    "        data = eval_data_onehot[index].as_in_context(ctx)\n",
    "        data = data.reshape(((1,) + data.shape))\n",
    "        \n",
    "        target = eval_label_onehot[index].as_in_context(ctx)\n",
    "        target = mx.nd.split(data=target, axis=0, num_outputs=1, squeeze_axis=True)\n",
    "\n",
    "        output = model.forward(data, data.shape[1])\n",
    "        output = output.reshape((target.shape[0], 1))\n",
    "        \n",
    "        L = mx.nd.softmax_cross_entropy(output, target)\n",
    "\n",
    "        total_loss += mx.nd.sum(L).asscalar()\n",
    "\n",
    "    return total_loss/num_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "def train(print_every=500):\n",
    "    start = time.time()\n",
    "    for epoch in range(1, num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for index in range(num_train):\n",
    "            \n",
    "            hidden = model.stack.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=ctx)\n",
    "            with autograd.record():\n",
    "#                 index = random.randint(0, num_train_data - 1)\n",
    "                data = train_data_onehot[index].as_in_context(ctx)\n",
    "                # since we do not have a batch size we'll reshape it to a batch_size of 1.\n",
    "                data = data.reshape(((1,) + data.shape))                \n",
    "\n",
    "                target = train_label_onehot[index].as_in_context(ctx)\n",
    "                target = mx.nd.split(data=target, axis=0, num_outputs=1, squeeze_axis=True)\n",
    "#                 print('target_shape.', target.shape)\n",
    "                \n",
    "                output = model.forward(data, data.shape[1])\n",
    "#                 print('forward_output.shape', output.shape)\n",
    "                \n",
    "                output = output.reshape((target.shape[0], 1))\n",
    "    \n",
    "#                 print('forward_output.reshape.shape', output.shape)\n",
    "                \n",
    "                softmax_out = mx.nd.softmax(output, axis=0)\n",
    "                L = mx.nd.softmax_cross_entropy(data=output, label=target)\n",
    "                L.backward()\n",
    "            \n",
    "            trainer.step(batch_size)\n",
    "\n",
    "            total_loss += L.asscalar()\n",
    "        \n",
    "        val_loss = eval()\n",
    "            \n",
    "        losses.append(total_loss)\n",
    "        print('[Epoch %d] Training loss=%f, Val loss=%f, time=%s'%(epoch, total_loss/num_train, val_loss, timeSince(start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
